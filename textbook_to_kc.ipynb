{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqADvv2hwefb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "from nltk.tokenize import TextTilingTokenizer\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### navigate to where your textbook is"
      ],
      "metadata": {
        "id": "NBXzUC3Faj7-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os.chdir('/content/drive/My Drive')"
      ],
      "metadata": {
        "id": "4GRCp0zdwmQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The format of textbook.csv is just one chapter copied into each row. So this is n x 1 with no header in the first cell. First cell = first chapter."
      ],
      "metadata": {
        "id": "ZmJuRsG5amjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "df = pd.read_csv(\"textbook.csv\", header = None)\n",
        "print(len(df))"
      ],
      "metadata": {
        "id": "-1W-REexxNEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "yWvJroAZ9GPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade google-generativeai\n",
        "import google.generativeai as genai\n",
        "from tqdm import tqdm\n",
        "import os"
      ],
      "metadata": {
        "id": "RyhTSdqjBYtg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Insert Gemini API Key and select model"
      ],
      "metadata": {
        "id": "xV6zjC4vbN8q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GEMINI_API_KEY = \"\"\n",
        "genai.configure(api_key=GEMINI_API_KEY)\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel(\"gemini-2.0-flash\")"
      ],
      "metadata": {
        "id": "4pVgPQgdBjSX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### In case extraction does not run through (e.g your notebook stops prematurely after 20 minutes), save all the outputs as txt files in a directory so if the txt exists it does not need to be regenerated.\n",
        "\n"
      ],
      "metadata": {
        "id": "wxfD84ojbUBf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result_directory = \"/content/drive/MyDrive/gemini_labels_KClist_new\"\n",
        "os.makedirs(result_directory, exist_ok=True)"
      ],
      "metadata": {
        "id": "2HCM91vCxx3r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feel Free to Edit Prompt as you see fit"
      ],
      "metadata": {
        "id": "ecajcjnXbZSH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def get_kc_label(segment, chapter_idx, segment_idx):\n",
        "    filename = os.path.join(result_directory, f\"{chapter_idx}_{segment_idx}_kc.txt\")\n",
        "\n",
        "    if os.path.exists(filename):\n",
        "        return  # Already processed\n",
        "\n",
        "    print(f\"Processing Chapter {chapter_idx}, Segment {segment_idx}...\")\n",
        "\n",
        "    prompt = f\"Summarize in 1–4 words the knowledge component described in this text:\\n\\n\\\"\\\"\\\"\\n{segment}\\n\\\"\\\"\\\"\"\n",
        "\n",
        "    sleep(5)  # Prevent rate limit\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        label = response.text.strip()\n",
        "    except Exception as e:\n",
        "        print(f\"Error at Chapter {chapter_idx}, Segment {segment_idx}: {e}\")\n",
        "        label = \"error\"\n",
        "\n",
        "    with open(filename, \"w\") as f:\n",
        "        f.write(label)"
      ],
      "metadata": {
        "id": "NPTEFamOyB4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## chunks text into appropriate kcs through topic segmentation\n",
        "def process_all_chapters(df):\n",
        "    for chapter_idx in range(len(df)):\n",
        "        chapter_text = df.iloc[chapter_idx, 0]\n",
        "\n",
        "        try:\n",
        "            segments = tt.tokenize(chapter_text)\n",
        "        except Exception as e:\n",
        "            print(f\"Skipping Chapter {chapter_idx} due to error: {e}\")\n",
        "            continue\n",
        "\n",
        "        for segment_idx, segment in enumerate(segments):\n",
        "            get_kc_label(segment.strip(), chapter_idx, segment_idx)"
      ],
      "metadata": {
        "id": "zhVTfnbNFQXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "process_all_chapters(df)"
      ],
      "metadata": {
        "id": "Th5_nhr4Ofp9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from nltk.tokenize import TextTilingTokenizer\n",
        "## w and k are hyperparameters that govern kc fineness\n",
        "tt = TextTilingTokenizer(w=50, k=170)\n",
        "result_directory = \"/content/drive/MyDrive/gemini_labels_KClist_new\"\n",
        "\n",
        "rows = []\n",
        "\n",
        "for chapter_idx in range(len(df)):\n",
        "    chapter_text = df.iloc[chapter_idx, 0]\n",
        "\n",
        "    try:\n",
        "        segments = tt.tokenize(chapter_text)\n",
        "    except Exception as e:\n",
        "        print(f\"Skipping Chapter {chapter_idx} due to error: {e}\")\n",
        "        continue\n",
        "\n",
        "    for segment_idx, segment in enumerate(segments):\n",
        "        label_path = os.path.join(result_directory, f\"{chapter_idx}_{segment_idx}_kc.txt\")\n",
        "\n",
        "        if os.path.exists(label_path):\n",
        "            with open(label_path, \"r\") as f:\n",
        "                kc_label = f.read().strip()\n",
        "        else:\n",
        "            kc_label = \"missing\"\n",
        "\n",
        "        rows.append({\n",
        "            \"chapter_num\": chapter_idx,\n",
        "            \"segment_text\": segment.strip(),\n",
        "            \"kc_label\": kc_label\n",
        "        })\n",
        "\n",
        "# Save CSV\n",
        "output_csv_path = os.path.join(result_directory, \"all_chapter_segments_with_kc.csv\")\n",
        "pd.DataFrame(rows).to_csv(output_csv_path, index=False)"
      ],
      "metadata": {
        "id": "970tjSt0Fdqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### This second run checks chunks are relevant for code and lets you filter out parts of textbook that shouldn't be a kc (e.g. history of the name \"python\")"
      ],
      "metadata": {
        "id": "wXlnsI9Zbrf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from time import sleep\n",
        "\n",
        "def is_relevant_to_solving(prompt_text):\n",
        "    prompt = f\"\"\"\n",
        "You are helping filter textbook content to assist students working on programming assignments.\n",
        "\n",
        "Here is a piece of text. Only answer with 0 or 1. Is it directly helpful for writing a specific piece of code (e.g., solving a homework problem)?\n",
        "\n",
        "- Answer **1** if the text contains a clear, concise explanation of a concept, coding principle, or logic that would directly help a student write or debug code for a specific problem.\n",
        "- Answer **0** if the text provides general background, historical context, vague conceptual discussion, mismatched code/text, or anything not immediately actionable for solving a programming task.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"{prompt_text}\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "    try:\n",
        "        sleep(5)\n",
        "        response = model.generate_content(prompt)\n",
        "        answer = response.text.strip().lower()\n",
        "\n",
        "        if \"1\" in answer:\n",
        "            print(\"1\")\n",
        "            return 1\n",
        "        elif \"0\" in answer:\n",
        "            print(\"0\")\n",
        "            return 0\n",
        "\n",
        "        else:\n",
        "            return -1  # unclear result\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return -1\n",
        "\n",
        "kc_df = pd.read_csv(os.path.join(result_directory, \"all_chapter_segments_with_kc.csv\"))\n",
        "\n",
        "relevant_flags = []\n",
        "\n",
        "for i, row in kc_df.iterrows():\n",
        "    segment = row['segment_text']\n",
        "    print(f\"Checking relevance for row {i}...\")\n",
        "    relevant = is_relevant_to_solving(segment)\n",
        "    relevant_flags.append(relevant)\n",
        "\n",
        "kc_df['relevant'] = relevant_flags\n",
        "\n",
        "kc_df.to_csv(os.path.join(result_directory, \"all_chapter_segments_with_kc_and_relevance.csv\"), index=False)\n",
        "print(\"✅ Relevance column added and CSV saved.\")\n"
      ],
      "metadata": {
        "id": "biGnyTMOFhTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#filters a final csv with only relevant items\n",
        "\n",
        "input_csv = os.path.join(result_directory, \"all_chapter_segments_with_kc_and_relevance.csv\")\n",
        "df = pd.read_csv(input_csv)\n",
        "\n",
        "relevant_df = df[df['relevant'] == 1].copy()\n",
        "\n",
        "output_csv = os.path.join(result_directory, \"relevant_chapter_segments_only.csv\")\n",
        "relevant_df.to_csv(output_csv, index=False)\n",
        "\n",
        "print(f\"Saved filtered CSV with {len(relevant_df)} relevant segments to:\\n{output_csv}\")"
      ],
      "metadata": {
        "id": "HXX0NT_zHe8I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y7Kf7kRSMtCk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
